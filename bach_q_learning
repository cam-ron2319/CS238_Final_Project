import numpy as np
import torch
import random
import torch.nn as nn
from bach import LSTMModel, train_lstm_model, predict_sequence
from midi_tools import MidiProcessor


class BachMelodyQLearning:
    def __init__(self, num_notes=89, num_durations=15):
        """
        Simple Q-learning for melody generation

        Args:
        - num_notes: Total number of possible notes (including pauses)
        - num_durations: Total number of possible note durations
        """
        self.num_notes = num_notes
        self.num_durations = num_durations

        self.state_space = 3 * self.num_notes * self.num_durations
        self.action_space = self.num_notes * self.num_durations

        # State, action pair -> (cur_note, cur_dur), (next_note, next_dur)
        # Q-table: (current_note, current_duration, next_note, next_duration)
        self.q_table = np.zeros((self.state_space, self.action_space))

        # Learning parameters
        self.learning_rate = 0.1
        self.discount_factor = 0.95
        self.exploration_rate = 1.0
        self.min_exploration_rate = 0.01
        self.exploration_decay = 0.995
        self.hidden_size = 128
        self.output_size = num_notes + num_durations  # Output will be note + duration
        self.input_size = num_notes + num_durations  # Input size is also note + duration

        self.LSTM_model = None

    def melody_to_one_hot(self, melody_sequence, num_notes=89, num_durations=15):
        """
        Converts a melody sequence to a one-hot tensor for LSTM input.

        Args:
            melody_sequence: List of (note, duration) tuples or tensor
            num_notes: Number of unique notes
            num_durations: Number of unique durations

        Returns:
            One-hot encoded tensor for LSTM input
        """
        # Ensure melody_sequence is in a consistent format
        if torch.is_tensor(melody_sequence):
            melody_sequence = melody_sequence.numpy().tolist()

        # Prepare one-hot encodings
        one_hot_pause = np.zeros(num_notes + num_durations)
        one_hot_pause[0] = 1  # Pause is the first entry

        one_hot_notes = [np.zeros(num_notes) for _ in range(num_notes)]
        for i in range(num_notes - 1):
            one_hot_notes[i][i + 1] = 1  # Note indices start at 1

        # Initialize one-hot encodings for durations
        one_hot_durations = [np.zeros(num_durations) for _ in range(num_durations)]
        for i in range(num_durations - 1):
            one_hot_durations[i][i + 1] = 1  # Duration indices start at 1

        # Convert melody sequence to one-hot
        sequence = []
        for elem in melody_sequence:
            # Handle different input formats
            if isinstance(elem, (list, tuple, np.ndarray)):
                # Unpack note and duration, handling potential nested structures
                note = elem[0] if isinstance(elem[0], (int, float, np.integer)) else elem[0][0]
                duration = elem[1] if isinstance(elem[1], (int, float, np.integer)) else elem[1][0]
            else:
                # If the input is not in expected format, use a default pause
                note, duration = 0, 0

            # Robust type conversion and bounds checking
            note = int(min(max(note, 0), num_notes - 1))
            duration = int(min(max(duration, 0), num_durations - 1))

            # Handle pause or invalid input
            if note == 0 and duration == 0:
                sequence.append(one_hot_pause)
            else:
                # Combine note and duration one-hot encodings
                note_duration = np.concatenate([one_hot_notes[note], one_hot_durations[duration]])
                sequence.append(note_duration)

        # Convert to tensor and add batch dimension
        return torch.tensor(sequence, dtype=torch.float32).unsqueeze(0)

    def set_up_LSTM_model(self, model):
        self.LSTM_model = model

    def get_rewards(self, sequence):
        one_hot_sequence = self.melody_to_one_hot(sequence)
        pred_pitch_probs, pred_duration_probs = predict_sequence(one_hot_sequence)
        # change this to take the log of the probabilities
        # Take the log of the probabilities
        log_pitch_probs = np.log(pred_pitch_probs)
        log_duration_probs = np.log(pred_duration_probs)
        reward = np.mean(log_pitch_probs + log_duration_probs)  # Example: average log-probability
        return reward


    def select_action(self, cur_state, available_actions):
        """
        epsilon greedy action selection
        :param cur_state: (note, duration) pair
        :param available_actions: list of (next_note, next_duration) pairs
        :return:
        """
        # Epsilon-greedy approach
        if random.random() < self.exploration_rate:
            return random.choice(available_actions)
        else:
            best_action = None
            max_q_val = float("-inf")
            for next_note, next_duration in available_actions:
                q_val = self.q_table[cur_state, next_note, next_duration]
                if q_val > max_q_val:
                    max_q_val = q_val
                    best_action = (next_note, next_duration)
            return best_action

    def Q_learning_update(self, cur_state, action, reward):
        q_val = self.q_table[cur_state, action]
        max_next_q_val = np.max(self.q_table[cur_state, :])
        q_val += self.learning_rate * (reward + self.discount_factor * max_next_q_val - q_val)

        self.q_table[cur_state, action] = q_val

    def train(self, epochs=100, sequence_len=32):
        for epoch in range(epochs):
            # initialize a random start point
            current_state = np.array([np.zeros(104), np.zeros(104), np.zeros(104)])
            # initialize a fixed starting point
            current_state[0][49] = 1
            current_state[0][93] = 1

            current_state[1][51] = 1
            current_state[1][93] = 1

            current_state[2][53] = 1
            current_state[2][93] = 1

            #next_note, next_duration = random.choice(range(self.num_notes - 1)), random.choice(range(self.num_durations-1))

            sequence = current_state

            for _ in range(sequence_len - 1):
                possible_actions = [(next_note, next_duration)
                                    for next_note in range(self.num_notes)
                                    for next_duration in range(self.num_durations)]
                action = self.select_action(current_state, possible_actions)
                np.delete(sequence, 0)
                np.append(sequence, action)

                sequence = [np.array(item).reshape(1, 104) for item in sequence]
                # Stack the sequence into a single numpy array of shape (3, 104)
                stacked_sequence = np.vstack(sequence)
                # Convert to a PyTorch tensor and add the batch dimension to make it (1, 3, 104)
                tensor_sequence = torch.tensor(stacked_sequence, dtype=torch.float32).unsqueeze(0)
                reward = self.get_rewards(tensor_sequence)

                self.Q_learning_update(current_state, action, reward)
                current_state = sequence
        return self.q_table

    def generate_melody(self, sequence_length=32):
        """
        Generate a melody using learned Q-values

        Args:
        - sequence_length: Number of notes in the melody
        - initial_state: Optional starting state

        Returns:
        - Generated melody sequence
        """
        # initialize a random start point
        current_state = [np.zeros(104), np.zeros(104), np.zeros(104)]
        # initialize a fixed starting point
        current_state[0][49] = 1
        current_state[0][93] = 1

        current_state[1][51] = 1
        current_state[1][93] = 1

        current_state[2][53] = 1
        current_state[2][93] = 1

        melody_sequence = []

        for _ in range(sequence_length):
            # take the max Q-value
            best_action = np.argmax(self.q_table[current_state])
            # Choose action
            melody_sequence.append(best_action)

            # Update current state
            current_state.append(best_action)
            np.delete(current_state, 0)

        print(melody_sequence)
        return melody_sequence

    def random_melody(self, sequence_length=32):
        random_melody_sequence = []
        for _ in range(sequence_length):
            available_actions = [
                (next_note, next_duration)
                for next_note in range(self.num_notes)
                for next_duration in range(self.num_durations)
            ]

            action = random.choice(available_actions)
            random_melody_sequence.append(action)
        return random_melody_sequence

def main():
    # Load pre-trained LSTM model (assuming you have the previous code)
    midi_processor = MidiProcessor()
    X, y = midi_processor.process_for_lstm("/Users/ianlasic/Downloads/988-v01.mid")

    # Convert data to tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.float32)

    # Initialize and train LSTM model
    model = LSTMModel(input_size=104, hidden_size=128, output_size=104)
    criterion = nn.BCEWithLogitsLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    train_lstm_model(X_tensor, y_tensor, model, criterion, optimizer, epochs=1)

    # Initialize Q-learning agent
    q_learning_agent = BachMelodyQLearning()
    q_learning_agent.set_up_LSTM_model(model)

    # Train Q-learning agent
    q_table = q_learning_agent.train()

    # Generate melody
    generated_melody = q_learning_agent.generate_melody()
    print("Generated Melody:", generated_melody)

    random_melody = q_learning_agent.random_melody()
    print("Random Melody:", random_melody)


if __name__ == '__main__':
    main()
